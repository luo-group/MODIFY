# Sample configuration file for evcouplings monomer protein prediction pipeline.
# This file determines all aspects of the computation:
# - which compute environment to use
# - which stages of the pipeline to run
# - what the settings for each of the stages are

# Minimal settings required before this configuration can be executed:
# - set your environment, paths to tools and databases (at the end of this file)
# - under "global", set prefix and sequence_id
# - run it! :)

# Configuration rules:
# 1) Global settings override settings for stages
# 2) Outputs of a stage are merged into "global" and fed into the input of subsequent stages
#    (e.g., the alignment_file output of align will be used by the alignment_file input of couplings)
# 3) All settings are explicitly specified here. No hidden defaults in code.
# 4) Each stage is also passed the parameters in the "databases" and "tools" sections

pipeline: protein_monomer
stages:
- align
- couplings

# Global job settings (which protein, region). These will override settings of the same name in each of the stages.
# These are typically the settings you want to modify for each of your jobs, together with some settings in the align stage.
global:
    # mandatory output prefix of the job (e.g. output/HRAS will store outputs in folder "output", using files prefixed with "HRAS")
  prefix: # path to output prefix
  sequence_id: TARGET
  sequence_file: # path to input alignment file
  region:
  theta: 0.8
  cpu: 60
batch:
align:
# Alternative protocol: reuse existing alignment and apply postprocessing to generate alignment that is consistent
# with pipeline requirements. Uncomment, and comment all values in align section above to enable the "existing" protocol
  protocol: existing
  prefix:
  input_alignment: # path to input alignment file
  sequence_id: TARGET
  first_index:
  compute_num_effective_seqs: false
  theta:
  seqid_filter:
  minimum_sequence_coverage: 50
  minimum_column_coverage: 0
  extract_annotation: true
couplings:
    # current options: 
    # - standard (model inference using plmc)
    # - mean_field (mean field direct coupling analysis, see below)
  protocol: standard
  iterations: 100
  alphabet:
  ignore_gaps: true
  lambda_J: 0.01
  lambda_J_times_Lq: true
  lambda_h: 0.01
  lambda_group:
  scale_clusters:
  reuse_ecs: false
  min_sequence_distance: 6
  scoring_model: logistic_regression
management:
    # URI of database
  database_uri:
  job_name:
  archive: [target_sequence_file, statistics_file, alignment_file, frequencies_file,
    ec_file, ec_longrange_file, model_file, enrichment_file, evzoom_file, enrichment_pml_files,
    ec_lines_pml_file, contact_map_files, ec_compared_all_file, ec_compared_longrange_file,
    remapped_pdb_files, mutations_epistatic_pml_files, mutation_matrix_file, mutation_matrix_plot_files,
    secondary_structure_pml_file, folding_ec_file, folded_structure_files, folding_ranking_file,
    folding_comparison_file, folding_individual_comparison_files, ec_lines_compared_pml_file,
    pdb_structure_hits_file, sec_struct_file]
environment:
    # current options for engine: lsf, local, slurm (for local, only set cores and leave all other fields blank)
    # If your batch engine of choice (e.g. SGE, Torque) is not available yet, please consider contributing by
    # implementing it and submitting a pull request!
    # Note that "cores" will override the "cpu" parameter for "global"
  engine:
  queue:
  cores: 1
  memory:
  time:
  configuration:
databases:
    # Sequence databases (only download the ones you want to use). You can also specify arbitrary databases in FASTA format
    # using a database name of your choice here)
  uniprot: /n/groups/marks/databases/jackhmmer/uniprot/uniprot_current.o2.fasta
  uniref100: /n/groups/marks/databases/jackhmmer/uniref100/uniref100_current.o2.fasta
  uniref90: /n/groups/marks/databases/jackhmmer/uniref90/uniref90_current.o2.fasta
  sequence_download_url: http://rest.uniprot.org/uniprot/{}.fasta
  pdb_mmtf_dir:
tools:
  plmc: # path to bin/plmc
  hmmbuild: # path to bin/hmmbuild
  hmmsearch: # path to bin/hmmsearch
  hhfilter: # path to bin/hhfilter
